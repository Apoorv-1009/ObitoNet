{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinaylanka/miniconda3/envs/obitonet/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pointnet2_ops import pointnet2_utils\n",
    "from knn_cuda import KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load a ply point cloud, print it, and render it\n",
      "\u001b[1;33m[Open3D WARNING] Read PLY failed: unable to open file: ../dataset/OldDataset/Barn_is/Barn/Barn01.ply\u001b[0;m\n",
      "PointCloud with 0 points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RPly: Unable to open file\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to stack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m voxel_grid \u001b[38;5;241m=\u001b[39m o3d\u001b[38;5;241m.\u001b[39mgeometry\u001b[38;5;241m.\u001b[39mVoxelGrid\u001b[38;5;241m.\u001b[39mcreate_from_point_cloud(pcd,\n\u001b[1;32m      7\u001b[0m                                                         voxel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m      8\u001b[0m voxels \u001b[38;5;241m=\u001b[39m voxel_grid\u001b[38;5;241m.\u001b[39mget_voxels()  \u001b[38;5;66;03m# returns list of voxels\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvoxels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m colors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28mlist\u001b[39m(vx\u001b[38;5;241m.\u001b[39mcolor \u001b[38;5;28;01mfor\u001b[39;00m vx \u001b[38;5;129;01min\u001b[39;00m voxels))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# print(indices[0:10])\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# print(voxel_grid)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/obitonet/lib/python3.10/site-packages/numpy/core/shape_base.py:445\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[1;32m    443\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [asanyarray(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arrays:\n\u001b[0;32m--> 445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneed at least one array to stack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    447\u001b[0m shapes \u001b[38;5;241m=\u001b[39m {arr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays}\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shapes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to stack"
     ]
    }
   ],
   "source": [
    "print(\"Load a ply point cloud, print it, and render it\")\n",
    "path = \"../dataset/OldDataset/Barn_is/Barn/Barn01.ply\"\n",
    "pcd = o3d.io.read_point_cloud(path)\n",
    "print(pcd)\n",
    "\n",
    "voxel_grid = o3d.geometry.VoxelGrid.create_from_point_cloud(pcd,\n",
    "                                                        voxel_size=0.01)\n",
    "voxels = voxel_grid.get_voxels()  # returns list of voxels\n",
    "indices = np.stack(list(vx.grid_index for vx in voxels))\n",
    "colors = np.stack(list(vx.color for vx in voxels))\n",
    "# print(indices[0:10])\n",
    "# print(voxel_grid)\n",
    "o3d.visualization.draw_geometries([voxel_grid])\n",
    "\n",
    "voxel_grid = o3d.geometry.VoxelGrid.create_from_point_cloud(pcd,\n",
    "                                                    voxel_size=0.2)\n",
    "voxels = voxel_grid.get_voxels()  # returns list of voxels\n",
    "indices = np.stack(list(vx.grid_index for vx in voxels))\n",
    "colors = np.stack(list(vx.color for vx in voxels))\n",
    "# print(indices[0:10])\n",
    "# print(voxel_grid)\n",
    "o3d.visualization.draw_geometries([voxel_grid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer (nn.Module):\n",
    "    def __init__(self, num_group, group_size):\n",
    "        super().__init__()\n",
    "        self.num_group = num_group\n",
    "        self.group_size = group_size\n",
    "        self.knn = KNN(k=self.group_size, transpose_mode=True)\n",
    "        self.mask_ratio = 0.6\n",
    "\n",
    "    def forward(self, xyz):\n",
    "            '''\n",
    "                input: B N 3\n",
    "                ---------------------------\n",
    "                output: B G M 3\n",
    "                center : B G 3\n",
    "            '''\n",
    "            batch_size, num_points, _ = xyz.shape\n",
    "            # fps the centers out\n",
    "            center = self.fps(xyz, self.num_group) # B G 3\n",
    "            # knn to get the neighborhood\n",
    "            _, idx = self.knn(xyz, center) # B G M\n",
    "            assert idx.size(1) == self.num_group\n",
    "            assert idx.size(2) == self.group_size\n",
    "            idx_base = torch.arange(0, batch_size, device=xyz.device).view(-1, 1, 1) * num_points\n",
    "            idx = idx + idx_base\n",
    "            idx = idx.view(-1)\n",
    "            neighborhood = xyz.view(batch_size * num_points, -1)[idx, :]\n",
    "            neighborhood = neighborhood.view(batch_size, self.num_group, self.group_size, 3).contiguous()\n",
    "            # normalize\n",
    "            neighborhood = neighborhood - center.unsqueeze(2)\n",
    "            return neighborhood, center\n",
    "    \n",
    "    def masking(self, center, noaug = False):\n",
    "        '''\n",
    "            center : B G 3\n",
    "            --------------\n",
    "            mask : B G (bool)\n",
    "        '''\n",
    "        B, G, _ = center.shape\n",
    "        # skip the mask\n",
    "        if noaug or self.mask_ratio == 0:\n",
    "            return torch.zeros(center.shape[:2]).bool()\n",
    "\n",
    "        self.num_mask = int(self.mask_ratio * G)\n",
    "\n",
    "        overall_mask = np.zeros([B, G])\n",
    "        for i in range(B):\n",
    "            mask = np.hstack([\n",
    "                np.zeros(G-self.num_mask),\n",
    "                np.ones(self.num_mask),\n",
    "            ])\n",
    "            np.random.shuffle(mask)\n",
    "            overall_mask[i, :] = mask\n",
    "        overall_mask = torch.from_numpy(overall_mask).to(torch.bool)\n",
    "\n",
    "        return overall_mask.to(center.device) # B G\n",
    "    \n",
    "    def fps(self, data, number):\n",
    "        '''\n",
    "            data B N 3\n",
    "            number int\n",
    "        '''\n",
    "        # print(number)\n",
    "        # print(\"yoyoyo\",data.scalar_type())\n",
    "        fps_idx = pointnet2_utils.furthest_point_sample(data, number) \n",
    "        fps_data = pointnet2_utils.gather_operation(data.transpose(1, 2).contiguous(), fps_idx).transpose(1,2).contiguous()\n",
    "        print(fps_data)\n",
    "        return fps_data\n",
    "    \n",
    "def farthest_point_sample(point, npoint):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        xyz: pointcloud data, [N, D]\n",
    "        npoint: number of samples\n",
    "    Return:\n",
    "        centroids: sampled pointcloud index, [npoint, D]\n",
    "    \"\"\"\n",
    "    N, D = point.shape\n",
    "    xyz = point[:,:3]\n",
    "    centroids = np.zeros((npoint,))\n",
    "    distance = np.ones((N,)) * 1e10\n",
    "    farthest = np.random.randint(0, N)\n",
    "    for i in range(npoint):\n",
    "        centroids[i] = farthest\n",
    "        centroid = xyz[farthest, :]\n",
    "        dist = np.sum((xyz - centroid) ** 2, -1)\n",
    "        mask = dist < distance\n",
    "        distance[mask] = dist[mask]\n",
    "        farthest = np.argmax(distance, -1)\n",
    "    point = point[centroids.astype(np.int32)]\n",
    "    return point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(32,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "class PointcloudScaleAndTranslate(object):\n",
    "    def __init__(self, scale_low=2. / 3., scale_high=3. / 2., translate_range=0.2):\n",
    "        self.scale_low = scale_low\n",
    "        self.scale_high = scale_high\n",
    "        self.translate_range = translate_range\n",
    "\n",
    "    def __call__(self, pc):\n",
    "        bsize = pc.size()[0]\n",
    "        for i in range(bsize):\n",
    "            xyz1 = np.random.uniform(low=self.scale_low, high=self.scale_high, size=[3])\n",
    "            xyz2 = np.random.uniform(low=-self.translate_range, high=self.translate_range, size=[3])\n",
    "            \n",
    "            pc[i, :, 0:3] = torch.mul(pc[i, :, 0:3], torch.from_numpy(xyz1).float().cuda()) + torch.from_numpy(xyz2).float().cuda()\n",
    "            \n",
    "        return pc\n",
    "\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        # data_transforms.PointcloudScale(),\n",
    "        # data_transforms.PointcloudRotate(),\n",
    "        # data_transforms.PointcloudRotatePerturbation(),\n",
    "        # data_transforms.PointcloudTranslate(),\n",
    "        # data_transforms.PointcloudJitter(),\n",
    "        # data_transforms.PointcloudRandomInputDropout(),\n",
    "        PointcloudScaleAndTranslate(),\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16384, 3)\n"
     ]
    }
   ],
   "source": [
    "path = \"../dataset/TanksAndTemples/pcd/Barn01.npy\"\n",
    "np_pcd = np.load(path)\n",
    "\n",
    "print(np_pcd.shape)\n",
    "tensor_pcd = torch.from_numpy(np.reshape(np_pcd, (1, np_pcd.shape[0], np_pcd.shape[1]))).to(device).cuda().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_pcd = train_transforms(tensor_pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  3.6213,  -3.8718, -30.1582],\n",
      "         [-18.8036,  -3.0308, -27.6767],\n",
      "         [ -3.9251,   4.7235, -23.9526],\n",
      "         ...,\n",
      "         [  2.2797,  -3.9714, -30.3885],\n",
      "         [  2.3024,  -3.9701, -28.3812],\n",
      "         [  3.6038,  -3.4454, -30.5889]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(tensor_pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  3.6213,  -3.8718, -30.1582],\n",
      "         [-18.8036,  -3.0308, -27.6767],\n",
      "         [ -3.9251,   4.7235, -23.9526],\n",
      "         [ -7.8995,  -3.7772, -31.2021],\n",
      "         [  4.4982,   5.7477, -31.1705],\n",
      "         [ -2.3236,  -2.9521, -24.9997],\n",
      "         [  1.6275,   1.9024, -26.8119],\n",
      "         [-13.8521,  -3.8867, -31.2086],\n",
      "         [ -2.0782,  -2.9137, -31.1197],\n",
      "         [ -9.7824,  -2.8209, -26.6483],\n",
      "         [  4.3347,   0.8771, -31.1768],\n",
      "         [ -5.4326,  -2.8684, -27.8820],\n",
      "         [  1.5097,  -2.3424, -26.8467],\n",
      "         [-13.9261,  -2.5668, -27.1863],\n",
      "         [ -1.9032,   1.0804, -25.0669],\n",
      "         [ -0.1743,   4.7108, -25.8325],\n",
      "         [  3.9415,   4.2433, -28.1107],\n",
      "         [-17.2273,  -3.3755, -31.1153],\n",
      "         [  3.7811,  -0.2665, -28.1048],\n",
      "         [ -5.6008,  -2.9063, -24.5514],\n",
      "         [-10.7339,  -2.8871, -29.7989],\n",
      "         [  0.6886,  -4.3652, -31.2128],\n",
      "         [ -4.8332,  -4.2503, -31.1903],\n",
      "         [ -2.0804,  -3.0258, -28.0697],\n",
      "         [ -4.1141,  -0.3909, -23.9580],\n",
      "         [-16.1150,  -3.3727, -28.6698],\n",
      "         [ -8.0111,  -2.8187, -28.7297],\n",
      "         [ -0.5993,  -1.1104, -25.7696],\n",
      "         [  0.7583,  -3.9569, -28.6947],\n",
      "         [  3.8630,   3.3437, -30.6300],\n",
      "         [  4.2063,  -1.6455, -31.1920],\n",
      "         [  3.1296,  -4.0341, -27.6844]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test, centers = tokenizer.forward(tensor_pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 128, 3])\n"
     ]
    }
   ],
   "source": [
    "print(test.shape)\n",
    "\n",
    "# print(test[0].shape)\n",
    "# print(test[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_patch = test.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "print(pcd_patch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "\n",
    "# Pass xyz to Open3D.o3d.geometry.PointCloud and visualize\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(pcd_patch[0][31])\n",
    "o3d.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_masked_pos = tokenizer.masking(centers, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False,  True,  True,  True,  True, False,  True, False,  True, False,\n",
      "          True, False,  True, False, False,  True, False,  True, False,  True,\n",
      "          True,  True,  True,  True,  True, False, False, False,  True,  True,\n",
      "         False,  True]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(bool_masked_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_input_tokens = self.encoder(neighborhood)  #  B G C\n",
    "\n",
    "batch_size, seq_len, C = group_input_tokens.size()\n",
    "\n",
    "x_vis = group_input_tokens[~bool_masked_pos].reshape(batch_size, -1, C)\n",
    "# add pos embedding\n",
    "# mask pos center\n",
    "masked_center = center[~bool_masked_pos].reshape(batch_size, -1, 3)\n",
    "pos = self.pos_embed(masked_center)\n",
    "\n",
    "# transformer\n",
    "x_vis = self.blocks(x_vis, pos)\n",
    "x_vis = self.norm(x_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5abeb0e41b8041228e059a4f0c7753ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cats-image.py:   0%|          | 0.00/2.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7f2ac310984f3bb8516080f5a792dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/173k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1bdd88d9214a4a9e4bca529b186961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138ef8f7686c4e1994632e32128ad5f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67654209acb4cef9008e0735ff6b883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73fe863b2e6240f6b5ff39b1d15ad23a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[1, 197, 768]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, ViTModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"huggingface/cats-image\", trust_remote_code=True)\n",
    "image = dataset[\"test\"][\"image\"][0]\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "inputs = image_processor(image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obitonet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
